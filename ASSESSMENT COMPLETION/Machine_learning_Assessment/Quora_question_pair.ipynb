{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quora question pair similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arunk\\AppData\\Local\\Temp\\ipykernel_12736\\873028983.py:2: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv('test.csv')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df= pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         What is the step by step guide to invest in sh...\n",
       "1            What is the story of Kohinoor KohiNoor Diamond\n",
       "2         How can I increase the speed of my internet co...\n",
       "3          Why am I mentally very lonely How can I solve it\n",
       "4         Which one dissolve in water quikly sugar salt ...\n",
       "                                ...                        \n",
       "404285    How many keywords are there in the Racket prog...\n",
       "404286             Do you believe there is life after death\n",
       "404287                                     What is one coin\n",
       "404288    What is the approx annual cost of living while...\n",
       "404289                 What is like to have sex with cousin\n",
       "Name: question1, Length: 404287, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['question1'].apply(lambda X: re.sub(\"[^A-Za-z1-9 ]\", \"\", X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.isnull().sum()\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Download the word tokenizer\n",
    "nltk.download('stopwords')  # Download the stopwords corpus\n",
    "nltk.download('wordnet')  # Download the WordNet lemmatizer\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Text Data Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords =nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocess the text data\n",
    "def process(text):\n",
    "    text = re.sub(\"[^A-Za-z1-9 ]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    clean_list = []\n",
    "    for token in tokens:\n",
    "        if token not in en_stopwords:\n",
    "            clean_list.append(lemmatizer.lemmatize(token))\n",
    "    return \" \".join(clean_list)\n",
    "\n",
    "df['question1'] = df['question1'].apply(process)\n",
    "df['question2'] = df['question2'].apply(process)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = df[['question1', 'question2']]\n",
    "y = df['is_duplicate']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "combined_data = X_train['question1'] + ' ' + X_train['question2']\n",
    "combined_data1 = X_test['question1'] + ' ' + X_test['question2']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Using LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7522817779316827\n",
      "                                                Question1   \n",
      "8067                                 play pokmon go korea  \\\n",
      "224279                     breathing treatment help cough   \n",
      "252452                  kellyanne conway annoying opinion   \n",
      "174039                       rate 11 review maruti baleno   \n",
      "384863                                good book marketing   \n",
      "...                                                   ...   \n",
      "37520       reason behind sudden end white collar tv show   \n",
      "75814                                        ever sex car   \n",
      "79271            recent research idea convex hull problem   \n",
      "25953   choose reading either fiction nonfiction rest ...   \n",
      "99888                                 iitpal app launched   \n",
      "\n",
      "                                                Question2  Similarity  \n",
      "8067                                 play pokmon go china           0  \n",
      "224279           help someone unconscious still breathing           0  \n",
      "252452  kellyanne conway really imply pay attention wo...           0  \n",
      "174039  career option one completing bachelor degree d...           0  \n",
      "384863                   best book ever written marketing           1  \n",
      "...                                                   ...         ...  \n",
      "37520                                favorite tv show end           0  \n",
      "75814   toip professor artificial intelligence canadia...           0  \n",
      "79271   indian feel controversial answer written niris...           0  \n",
      "25953   choose one fiction one nonfiction book read re...           0  \n",
      "99888                                      prepare iitjee           0  \n",
      "\n",
      "[80858 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training set\n",
    "vectorizer.fit(combined_data)\n",
    "X_train_tfidf = vectorizer.transform(combined_data)\n",
    "# Transform the testing set\n",
    "X_test_tfidf = vectorizer.transform(combined_data1)\n",
    "\n",
    "# Choose a supervised algorithm (e.g., Logistic Regression)\n",
    "model = LogisticRegression(max_iter=500)\n",
    "\n",
    "# Train the model on the TF-IDF features and labels\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate the similarity between the questions based on the predicted labels\n",
    "similarity = pd.DataFrame({'Question1': X_test['question1'], 'Question2': X_test['question2'], 'Similarity': y_pred})\n",
    "print(similarity)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Pair Cosine Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_reshaped = y_pred.reshape(1, -1)  # Reshape y_pred to have the same number of features as X_test_tfidf\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "X_test_tfidf_transposed = X_test_tfidf.T  # Transpose X_test_tfidf to have shape (87465, 80858)\n",
    "similarity_matrix = cosine_similarity(X_test_tfidf_transposed, y_pred_reshaped)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Using ComplementNB\n",
    "    - Pipeline \n",
    "    - RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Hyperparameters to be evaluated:\n",
      "{'clf__alpha': array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,\n",
      "       1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]),\n",
      " 'vect__max_df': (0.2, 0.4, 0.6, 0.8, 1.0),\n",
      " 'vect__min_df': (1, 3, 5, 10),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2)),\n",
      " 'vect__norm': ('l1', 'l2')}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Done in 1094.452s\n",
      "Best parameters combination found:\n",
      "clf__alpha: 1e-06\n",
      "vect__max_df: 0.4\n",
      "vect__min_df: 1\n",
      "vect__ngram_range: (1, 2)\n",
      "vect__norm: l1\n",
      "Accuracy of the best parameters using the inner CV of the random search: 0.793\n",
      "Accuracy on test set: 0.969\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from time import time\n",
    "import pandas as pd\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", TfidfVectorizer()),\n",
    "        (\"clf\", ComplementNB()),\n",
    "    ]\n",
    ")\n",
    "pipeline\n",
    "\n",
    "parameter_grid = {\n",
    "    \"vect__max_df\": (0.2, 0.4, 0.6, 0.8, 1.0),\n",
    "    \"vect__min_df\": (1, 3, 5, 10),\n",
    "    \"vect__ngram_range\": ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    \"vect__norm\": (\"l1\", \"l2\"),\n",
    "    \"clf__alpha\": np.logspace(-6, 6, 13),\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=parameter_grid,\n",
    "    n_iter=40,\n",
    "    random_state=0,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"Hyperparameters to be evaluated:\")\n",
    "pprint(parameter_grid)\n",
    "\n",
    "t0 = time()\n",
    "random_search.fit(combined_data, y_train)\n",
    "print(f\"Done in {time() - t0:.3f}s\")\n",
    "\n",
    "print(\"Best parameters combination found:\")\n",
    "best_parameters = random_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameter_grid.keys()):\n",
    "    print(f\"{param_name}: {best_parameters[param_name]}\")\n",
    "\n",
    "test_accuracy = random_search.score(combined_data, y_train)\n",
    "print(\n",
    "    \"Accuracy of the best parameters using the inner CV of \"\n",
    "    f\"the random search: {random_search.best_score_:.3f}\"\n",
    ")\n",
    "print(f\"Accuracy on test set: {test_accuracy:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - predicting the test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_copy = df_test.copy()\n",
    "\n",
    "df_test.dropna(inplace=True)\n",
    "df_test.drop_duplicates(inplace=True)\n",
    "df_test['question1'] = df_test['question1'].apply(process)\n",
    "df_test['question2'] = df_test['question2'].apply(process)\n",
    "test_combine = df_test['question1'] + ' ' + df_test['question2']\n",
    "y_pred = random_search.predict(test_combine)\n",
    "y_pred_df = pd.Series(y_pred, \"is_duplicate\")\n",
    "\n",
    "df_test.reset_index(drop=True, inplace=True)  # Reset the index of df_test\n",
    "\n",
    "result = pd.concat([df_test, y_pred_df], axis=1)\n",
    "\n",
    "print(df_test.shape)\n",
    "print(result.shape)\n",
    "print(result['is_duplicate'].count())\n",
    "print(result['test_id'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_submission = result[['test_id','is_duplicate']].set_index('test_id')\n",
    "result_submission\n",
    "result_submission.to_csv(\"result_submission.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The prefixes vect and clf are required to avoid possible ambiguities in the pipeline, but are not necessary for visualizing the results. Because of this, we define a function that will rename the tuned hyperparameters and improve the readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_param(param_name):\n",
    "    \"\"\"Remove components' prefixes in param_name.\"\"\"\n",
    "    if \"__\" in param_name:\n",
    "        return param_name.rsplit(\"__\", 1)[1]\n",
    "    return param_name\n",
    "\n",
    "\n",
    "cv_results = pd.DataFrame(random_search.cv_results_)\n",
    "cv_results = cv_results.rename(shorten_param, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can use a plotly.express.scatter to visualize the trade-off between scoring time and mean test score (i.e. “CV score”). Passing the cursor over a given point displays the corresponding parameters. Error bars correspond to one standard deviation as computed in the different folds of the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "param_names = [shorten_param(name) for name in parameter_grid.keys()]\n",
    "labels = {\n",
    "    \"mean_score_time\": \"CV Score time (s)\",\n",
    "    \"mean_test_score\": \"CV score (accuracy)\",\n",
    "}\n",
    "fig = px.scatter(\n",
    "    cv_results,\n",
    "    x=\"mean_score_time\",\n",
    "    y=\"mean_test_score\",\n",
    "    error_x=\"std_score_time\",\n",
    "    error_y=\"std_test_score\",\n",
    "    hover_data=param_names,\n",
    "    labels=labels,\n",
    ")\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        \"text\": \"trade-off between scoring time and mean test score\",\n",
    "        \"y\": 0.95,\n",
    "        \"x\": 0.5,\n",
    "        \"xanchor\": \"center\",\n",
    "        \"yanchor\": \"top\",\n",
    "    }\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can also use a plotly.express.parallel_coordinates to further visualize the mean test score as a function of the tuned hyperparameters. This helps finding interactions between more than two hyperparameters and provide intuition on their relevance for improving the performance of a pipeline.\n",
    "\n",
    "- We apply a math.log10 transformation on the alpha axis to spread the active range and improve the readability of the plot. A value \n",
    " on said X axis is to be understood as 10^X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "column_results = param_names + [\"mean_test_score\", \"mean_score_time\"]\n",
    "\n",
    "transform_funcs = dict.fromkeys(column_results, lambda x: x)\n",
    "# Using a logarithmic scale for alpha\n",
    "transform_funcs[\"alpha\"] = math.log10\n",
    "# L1 norms are mapped to index 1, and L2 norms to index 2\n",
    "transform_funcs[\"norm\"] = lambda x: 2 if x == \"l2\" else 1\n",
    "# Unigrams are mapped to index 1 and bigrams to index 2\n",
    "transform_funcs[\"ngram_range\"] = lambda x: x[1]\n",
    "\n",
    "fig = px.parallel_coordinates(\n",
    "    cv_results[column_results].apply(transform_funcs),\n",
    "    color=\"mean_test_score\",\n",
    "    color_continuous_scale=px.colors.sequential.Viridis_r,\n",
    "    labels=labels,\n",
    ")\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        \"text\": \"Parallel coordinates plot of text classifier pipeline\",\n",
    "        \"y\": 0.99,\n",
    "        \"x\": 0.5,\n",
    "        \"xanchor\": \"center\",\n",
    "        \"yanchor\": \"top\",\n",
    "    }\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total running time of the script: Done in 1094.452s\n",
    "\n",
    "Performing grid search...\n",
    "\n",
    "      * Hyperparameters to be evaluated:\n",
    "\n",
    "      {'clf__alpha': array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,\n",
    "            1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]),\n",
    "      'vect__max_df': (0.2, 0.4, 0.6, 0.8, 1.0),\n",
    "      'vect__min_df': (1, 3, 5, 10),\n",
    "      'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "      'vect__norm': ('l1', 'l2')}\n",
    "\n",
    "      * Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
    "\n",
    "      * Best parameters combination found:\n",
    "\n",
    "      clf__alpha: 1e-06\n",
    "      vect__max_df: 0.4\n",
    "      vect__min_df: 1\n",
    "      vect__ngram_range: (1, 2)\n",
    "      vect__norm: l1\n",
    "      \n",
    "      Accuracy of the best parameters using the inner CV of the random search: 0.793\n",
    "      Accuracy on test set: 0.969"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "parameter_grid = {\n",
    "    \"vect__max_df\": [0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "    \"vect__min_df\": [1, 3, 5, 10],\n",
    "    \"vect__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"vect__norm\": [\"l1\", \"l2\"],\n",
    "    \"clf__alpha\": np.logspace(-6, 6, 13),\n",
    "}\n",
    "\n",
    "for params in parameter_grid:\n",
    "    vectorizer.set_params(params)\n",
    "    X_train_tfidf = vectorizer.transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "    clf = ComplementNB()\n",
    "    clf.fit(X_train_tfidf, y_train)\n",
    "    y_pred = clf.predict(X_test_tfidf)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_params = params\n",
    "\n",
    "print(\"Best parameters combination found:\")\n",
    "print(best_params)\n",
    "print(\"Best accuracy:\", best_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
