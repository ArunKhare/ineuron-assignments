{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903fabc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a489738e",
   "metadata": {},
   "source": [
    "Practical Assignment\n",
    "\n",
    "Objective: - Predict Next Sequence\n",
    "To start with deep learning, the very basic project that you can build is to\n",
    "predict the next digit in a sequence.\n",
    "\n",
    "Dataset: - Create a sequence like a list of odd numbers and then build a model\n",
    "and train it to predict the next digit in the sequence.\n",
    "\n",
    "Task: - A simple neural network with 2 layers would be sufficient to build the\n",
    "model.\n",
    "\n",
    "Assignment Submission: - Only submit the Google Colab/Github link.(Make the\n",
    "Link Public)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a53232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d08d411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector =  [ 1, 2, 3, 4, 5]\n",
    "wt1 = [.5,.4,.3,.2,.1]\n",
    "wt2 = [2.1,0.8,.7,1,.2]\n",
    "\n",
    "X_test = [1, 2, 3, 4, 5]\n",
    "y_test = [6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6360f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computing dot product\n",
    "input_vector_array = np.array(input_vector)\n",
    "wt1_array =  np.array(wt1)\n",
    "wt2_array = np.array(wt2)\n",
    "dot_product_1 = sum(input_vector_array *wt1)\n",
    "dot_product_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54c9f272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simply apply np.dot function\n",
    "np.dot(input_vector,wt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "448149ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product_2 = np.dot(input_vector,wt2)\n",
    "dot_product_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18ef43fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input vector is more similar to wt2\n"
     ]
    }
   ],
   "source": [
    "#  dot product as a loose measurement of similarity between the vectors\n",
    "# Every time the multiplication result is 0, the final dot product will have a lower result\n",
    "if dot_product_1 < dot_product_2 :\n",
    "    print(f'input vector is more similar to wt2')\n",
    "else:\n",
    "    print(f'input vector is more similar to wt1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeebcc3",
   "metadata": {},
   "source": [
    "### Making first predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d36dd216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction result is: [0.9999796]\n"
     ]
    }
   ],
   "source": [
    "# two layer network\n",
    "# wrapping the vectors in numpy array\n",
    "input_vector = np.array(input_vector)\n",
    "weight_1 = np.array(wt2)\n",
    "bias = np.array([0.0])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return (1/(1+np.exp(-x)))\n",
    "def make_prediction(input_vector,weights,bias):\n",
    "    layer_1 = np.dot(input_vector, weights)+bias\n",
    "    layer_2 = sigmoid(layer_1)\n",
    "    return layer_2\n",
    "prediction = make_prediction(input_vector,weight_1,bias)\n",
    "print(f'The prediction result is: {prediction}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3802fcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction result is: [0.99981593]\n"
     ]
    }
   ],
   "source": [
    "# changing the value of input input vector\n",
    "input_vector = np.array([.2,.1,7,3,1])\n",
    "prediction = make_prediction(input_vector,weight_1,bias)\n",
    "print(f'The prediction result is: {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a229898",
   "metadata": {},
   "source": [
    "### Train your first neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4bbf696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction [0.99981593]; Error: [0.99963189]\n"
     ]
    }
   ],
   "source": [
    "# computing the prediction error\n",
    "target = 0\n",
    "\n",
    "mse = np.square(prediction-target)\n",
    "print(f'Prediction {prediction}; Error: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0fc01aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The derivative is [1.99963186]\n"
     ]
    }
   ],
   "source": [
    "# REDUCING ERROR\n",
    "#  error = np.square(x), which is a quadratic function.\n",
    "# To know which direction you should go to reduce the error, you’ll use the derivative\n",
    "# Another word for the derivative is gradient. Gradient descent is the name of the algorithm used to find the direction and the rate to update the network parameters.\n",
    "# error = np.square(prediction -target) = np.square(x) \n",
    "\n",
    "# By taking the derivative of this function, you want to know in what direction should you change x to bring the result of error to zero, thereby reducing the error.\n",
    "# derivative of error = 2x\n",
    " \n",
    "#     When it comes to your neural network, the derivative will tell you the direction you should take to update the weights variable\n",
    "# If +ve then you predicted too high, and you need to decrease the wts\n",
    "# If -ve you predicted too low and you need increase the wts\n",
    "\n",
    "derivative = 2*(prediction-target)\n",
    "print(f' The derivative is {derivative}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a76c2a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [3.03345868e-36]; Error: [9.20187158e-72]\n"
     ]
    }
   ],
   "source": [
    "# The result is 1.99 - decrease the wts\n",
    "# updating the weights\n",
    "weight_1 = weight_1-derivative\n",
    "prediction= make_prediction (input_vector,weight_1,bias)\n",
    "error = (prediction-target)**2\n",
    "print(f'Prediction: {prediction}; Error: {error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "718a60e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The network has two layers, and since each layer has its own functions, you’re dealing with a function composition. This means that the error function is still np.square(x), but now x is the result of another function.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here High increments aren’t ideal because you could keep going from point A straight to point B, never getting close to zero. To cope with that, you update the weights with a fraction of the derivative result.\n",
    "# To update weight with fraction of  the derivative result\n",
    "# Define fraction for updating weights, use alpha parameter, a Learning Rate.\n",
    "# Decreasing the LR, then the increment in wts is smaller.\n",
    "'''If you take the new weights and make a prediction with the first input vector, then you’ll see that now it makes a wrong prediction for that one. If your neural network makes a correct prediction for every instance in your training set, then you probably have an overfitted model, where the model simply remembers how to classify the examples instead of learning to notice features in the data.'''\n",
    "\"\"\"The network has two layers, and since each layer has its own functions, you’re dealing with a function composition. This means that the error function is still np.square(x), but now x is the result of another function.\"\"\"\n",
    "# Since now you have this function composition, to take the derivative of the error concerning the parameters, you’ll need to use the chain rule from calculus.\n",
    "# With the chain rule, you take the partial derivatives of each function, evaluate them, and multiply all the partial derivatives to get the derivative you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "02a80360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain rule = take the partial derivatives, evaluate, and multiply\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "derror_dprediction = 2*(prediction - target)\n",
    "layer_1= np.dot(input_vector,weight_1) +bias\n",
    "dprediction_dlayer_1 = sigmoid_deriv(layer_1)\n",
    "dlayer1_bias =1\n",
    "derror_dbias = (derror_dprediction * dprediction_dlayer_1 *dlayer1_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ce2b374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.84037432e-71])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derror_dbias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d430a6",
   "metadata": {},
   "source": [
    "# Using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "08d944c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.experimental.numpy as tnp\n",
    "tnp.experimental_enable_numpy_behavior()\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0f2c015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=tf.convert_to_tensor([1,2,3,4,5])\n",
    "y_train=tf.convert_to_tensor([6,7,8,9,10])\n",
    "X_valid= tf.convert_to_tensor([11,12,13,15,16])\n",
    "y_valid= tf.convert_to_tensor ([17,18,19,20])\n",
    "X_test = tf.convert_to_tensor ([21,22,23,24,25])\n",
    "y_test = tf.convert_to_tensor ([26,27,28,28,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6a3724f1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 5)                 30        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36\n",
      "Trainable params: 36\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "regressor = Sequential()\n",
    "regressor.add(Dense(5,input_shape=(5,),activation ='relu'))\n",
    "regressor.add(Dense(1,activation='sigmoid'))\n",
    "regressor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "36005c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "loss = tf.keras.losses.MeanSquaredError(name='mean_squared_error')\n",
    "regressor.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bd39fa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 5) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5), dtype=tf.float32, name='dense_2_input'), name='dense_2_input', description=\"created by layer 'dense_2_input'\"), but it was called on an input with incompatible shape (None,).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\arunk\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\arunk\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\arunk\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\arunk\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\arunk\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\arunk\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 228, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"sequential_2\" (type Sequential).\n    \n    Input 0 of layer \"dense_2\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received by layer \"sequential_2\" (type Sequential):\n      • inputs=tf.Tensor(shape=(None,), dtype=int32)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [86]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mregressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m X_test\u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m11\u001b[39m,\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m16\u001b[39m]\n\u001b[0;32m      3\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m regressor\u001b[38;5;241m.\u001b[39mpredcit(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileqk2ol_91.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\arunk\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\arunk\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\arunk\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\arunk\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\arunk\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\arunk\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 228, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"sequential_2\" (type Sequential).\n    \n    Input 0 of layer \"dense_2\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received by layer \"sequential_2\" (type Sequential):\n      • inputs=tf.Tensor(shape=(None,), dtype=int32)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "model = regressor.fit(X_train,y_train)\n",
    "\n",
    "y_pred = regressor.predcit(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb61b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
